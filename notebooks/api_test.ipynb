{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class RedditComment:\n",
    "    def __init__(self, comment):\n",
    "        self.id = comment.id\n",
    "        self.author = str(comment.author)\n",
    "        self.body = comment.body\n",
    "        self.score = comment.score\n",
    "        self.created_utc = comment.created_utc\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"author\": self.author,\n",
    "            \"body\": self.body,\n",
    "            \"score\": self.score,\n",
    "            \"created_utc\": self.created_utc\n",
    "        }\n",
    "\n",
    "class RedditPost:\n",
    "    def __init__(self, post):\n",
    "        self.title = post.title\n",
    "        self.score = post.score\n",
    "        self.url = post.url\n",
    "        self.created_utc = post.created_utc\n",
    "        self.author = str(post.author)\n",
    "        self.num_comments = post.num_comments\n",
    "        self.is_self = post.is_self\n",
    "        self.selftext = post.selftext if post.is_self else \"\"\n",
    "        self.subreddit = post.subreddit.display_name\n",
    "        self.comments = self.get_comments(post)\n",
    "\n",
    "    def get_comments(self, post, limit=10):\n",
    "        post.comments.replace_more(limit=0)  # Remove MoreComments objects\n",
    "        return [RedditComment(comment) for comment in post.comments.list()[:limit]]\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"title\": self.title,\n",
    "            \"score\": self.score,\n",
    "            \"url\": self.url,\n",
    "            \"created_utc\": self.created_utc,\n",
    "            \"author\": self.author,\n",
    "            \"num_comments\": self.num_comments,\n",
    "            \"is_self\": self.is_self,\n",
    "            \"selftext\": self.selftext,\n",
    "            \"subreddit\": self.subreddit,\n",
    "            \"comments\": [comment.to_dict() for comment in self.comments]\n",
    "        }\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_subreddit_posts(reddit, subreddit_name, limit=10, category='hot'):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    if category == 'hot':\n",
    "        return subreddit.hot(limit=limit)\n",
    "    elif category == 'new':\n",
    "        return subreddit.new(limit=limit)\n",
    "    elif category == 'top':\n",
    "        return subreddit.top(limit=limit)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid category. Choose 'hot', 'new', or 'top'.\")\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    limit = 20\n",
    "    category = 'hot'\n",
    "\n",
    "    posts = get_subreddit_posts(reddit, subreddit_name, limit, category)\n",
    "    reddit_posts = [RedditPost(post) for post in posts]\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{subreddit_name}_{category}_{timestamp}.json\"\n",
    "        # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {filename}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_subreddit_posts_last_5_years(reddit, subreddit_name):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=1*365)  # 5 years ago\n",
    "    \n",
    "    posts = []\n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break  # Stop if we've gone past 5 years\n",
    "        posts.append(RedditPost(post))\n",
    "        \n",
    "        # Reddit API has a rate limit, so we need to be careful\n",
    "        time.sleep(1.1)  # Sleep for 100ms between requests\n",
    "        \n",
    "        if len(posts) % 100 == 0:\n",
    "            print(f\"Fetched {len(posts)} posts...\")\n",
    "    \n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "\n",
    "    print(f\"Fetching posts from r/{subreddit_name} for the last 5 years...\")\n",
    "    reddit_posts = get_subreddit_posts_last_5_years(reddit, subreddit_name)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{subreddit_name}_last_5_years_{timestamp}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_subreddit_posts_last_5_years(reddit, subreddit_name,n_years=1):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=n_years*365)  # 5 years ago\n",
    "    \n",
    "    posts = []\n",
    "    pbar = tqdm(desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break  # Stop if we've gone past 5 years\n",
    "        posts.append(RedditPost(post))\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Reddit API has a rate limit, so we need to be careful\n",
    "        time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "    \n",
    "    pbar.close()\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    n_years = 1\n",
    "\n",
    "    print(f\"Fetching posts from r/{subreddit_name} for the last {n_years} years...\")\n",
    "    reddit_posts = get_subreddit_posts_last_5_years(reddit, subreddit_name)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{subreddit_name}_last_5_years_{timestamp}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    print(\"\\nSample of fetched data:\")\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def count_posts(reddit, subreddit_name, start_date):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    count = 0\n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def get_subreddit_posts_last_n_years(reddit, subreddit_name, n_years=.01):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=n_years*365)\n",
    "    \n",
    "    # First, count the posts\n",
    "    print(\"Counting posts...\")\n",
    "    total_posts = count_posts(reddit, subreddit_name, start_date)\n",
    "    print(f\"Found {total_posts} posts in the last {n_years} years.\")\n",
    "    \n",
    "    posts = []\n",
    "    pbar = tqdm(total=total_posts, desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break\n",
    "        posts.append(RedditPost(post))\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Reddit API has a rate limit, so we need to be careful\n",
    "        time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "    \n",
    "    pbar.close()\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    n_years = .01\n",
    "\n",
    "    print(f\"Fetching posts from r/{subreddit_name} for the last {n_years} years...\")\n",
    "    reddit_posts = get_subreddit_posts_last_n_years(reddit, subreddit_name, n_years)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{subreddit_name}_last_{n_years}_years_{timestamp}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    print(\"\\nSample of fetched data:\")\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def count_posts(reddit, subreddit_name, start_date, end_date):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    count = 0\n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break\n",
    "        if post_date <= end_date:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # First, count the posts\n",
    "    print(f\"Counting posts from {start_date.date()} to {end_date.date()}...\")\n",
    "    total_posts = count_posts(reddit, subreddit_name, start_date, end_date)\n",
    "    print(f\"Found {total_posts} posts in this period.\")\n",
    "    \n",
    "    posts = []\n",
    "    pbar = tqdm(total=total_posts, desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break\n",
    "        if post_date <= end_date:\n",
    "            posts.append(RedditPost(post))\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Reddit API has a rate limit, so we need to be careful\n",
    "        time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "    \n",
    "    pbar.close()\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    n_years = 1.5  # Change this to the desired number of years\n",
    "\n",
    "    end_date = datetime()\n",
    "    start_date = end_date - timedelta(days=n_years*365)\n",
    "\n",
    "    # Split into year intervals or less\n",
    "    current_start = start_date\n",
    "    while current_start < end_date:\n",
    "        current_end = min(current_start + timedelta(days=365), end_date)\n",
    "        \n",
    "        print(f\"Fetching posts from r/{subreddit_name} for the period: {current_start.date()} to {current_end.date()}\")\n",
    "        reddit_posts = get_subreddit_posts_for_period(reddit, subreddit_name, current_start, current_end)\n",
    "\n",
    "        # Create filename with date range\n",
    "        filename = f\"{subreddit_name}_{current_start.date()}_{current_end.date()}.json\"\n",
    "\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "        print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "        save_posts(reddit_posts, file_path)\n",
    "\n",
    "        print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "        # Print a sample of the data\n",
    "        print(\"\\nSample of fetched data:\")\n",
    "        for post in reddit_posts[:3]:\n",
    "            print(f\"Title: {post.title}\")\n",
    "            print(f\"Score: {post.score}\")\n",
    "            print(f\"URL: {post.url}\")\n",
    "            print(f\"Date: {datetime(post.created_utc)}\")\n",
    "            print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "            print(f\"Number of comments: {len(post.comments)}\")\n",
    "            if post.comments:\n",
    "                print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "            print(\"---\")\n",
    "\n",
    "        # Move to the next period\n",
    "        current_start = current_end + timedelta(seconds=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def count_posts(reddit, subreddit_name, start_date):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    count = 0\n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def get_subreddit_posts_for_period(reddit, subreddit_name, start_date):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # First, count the posts\n",
    "    print(f\"Counting posts from {start_date.date()} to now...\")\n",
    "    total_posts = count_posts(reddit, subreddit_name, start_date)\n",
    "    print(f\"Found {total_posts} posts in this period.\")\n",
    "    \n",
    "    posts = []\n",
    "    pbar = tqdm(total=total_posts, desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break\n",
    "        posts.append(RedditPost(post))\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Reddit API has a rate limit, so we need to be careful\n",
    "        time.sleep(0.15)  # Sleep for 150ms between requests\n",
    "    \n",
    "    pbar.close()\n",
    "    return posts\n",
    "\n",
    "def get_subreddit_posts_for_period(reddit, subreddit_name, start_date, max_posts=100000):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    posts = []\n",
    "    pbar = tqdm(total=max_posts, desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    for post in subreddit.new(limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if post_date < start_date or len(posts) >= max_posts:\n",
    "            break\n",
    "        posts.append(RedditPost(post))\n",
    "        pbar.update(1)\n",
    "        \n",
    "        time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Retrieved {len(posts)} posts. Oldest post date: {datetime.utcfromtimestamp(posts[-1].created_utc) if posts else 'N/A'}\")\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    n_years = 5  # Change this to the desired number of years\n",
    "\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=n_years*365)\n",
    "    \n",
    "    print(f\"Fetching posts from r/{subreddit_name} for the period: {start_date.date()} to {end_date.date()}\")\n",
    "    reddit_posts = get_subreddit_posts_for_period(reddit, subreddit_name, start_date)\n",
    "\n",
    "    # Create filename with date range\n",
    "    filename = f\"{subreddit_name}_{start_date.date()}_{end_date.date()}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_pushshift_posts(subreddit_name, start_date, end_date, limit=1000):\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    start_timestamp = int(start_date.timestamp())\n",
    "    end_timestamp = int(end_date.timestamp())\n",
    "    \n",
    "    params = {\n",
    "        \"subreddit\": subreddit_name,\n",
    "        \"after\": start_timestamp,\n",
    "        \"before\": end_timestamp,\n",
    "        \"size\": limit,\n",
    "        \"sort\": \"desc\",\n",
    "        \"sort_type\": \"created_utc\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    return data['data']\n",
    "\n",
    "def get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date, max_posts=100000):\n",
    "    posts = []\n",
    "    pbar = tqdm(total=max_posts, desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    current_date = end_date\n",
    "    while len(posts) < max_posts and current_date > start_date:\n",
    "        pushshift_posts = get_pushshift_posts(subreddit_name, start_date, current_date, limit=1000)\n",
    "        if not pushshift_posts:\n",
    "            break\n",
    "        \n",
    "        for post_data in pushshift_posts:\n",
    "            post_date = datetime.utcfromtimestamp(post_data['created_utc'])\n",
    "            if post_date < start_date or len(posts) >= max_posts:\n",
    "                break\n",
    "            \n",
    "            # Fetch full post data from Reddit API\n",
    "            full_post = reddit.submission(id=post_data['id'])\n",
    "            posts.append(RedditPost(full_post))\n",
    "            pbar.update(1)\n",
    "            \n",
    "            time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "        \n",
    "        current_date = datetime.utcfromtimestamp(pushshift_posts[-1]['created_utc'])\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Retrieved {len(posts)} posts. Oldest post date: {datetime.utcfromtimestamp(posts[-1].created_utc) if posts else 'N/A'}\")\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    n_years = 5  # Change this to the desired number of years\n",
    "\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=n_years*365)\n",
    "    \n",
    "    print(f\"Fetching posts from r/{subreddit_name} for the period: {start_date.date()} to {end_date.date()}\")\n",
    "    reddit_posts = get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date)\n",
    "\n",
    "    # Create filename with date range\n",
    "    filename = f\"{subreddit_name}_{start_date.date()}_{end_date.date()}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    print(\"\\nSample of fetched data:\")\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_pushshift_posts(subreddit_name, start_date, end_date, limit=1000):\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    start_timestamp = int(start_date.timestamp())\n",
    "    end_timestamp = int(end_date.timestamp())\n",
    "    \n",
    "    params = {\n",
    "        \"subreddit\": subreddit_name,\n",
    "        \"after\": start_timestamp,\n",
    "        \"before\": end_timestamp,\n",
    "        \"size\": limit,\n",
    "        \"sort\": \"desc\",\n",
    "        \"sort_type\": \"created_utc\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        data = response.json()\n",
    "        return data.get('data', [])\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from Pushshift: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date, max_posts=100000):\n",
    "    posts = []\n",
    "    pbar = tqdm(total=max_posts, desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    current_date = end_date\n",
    "    while len(posts) < max_posts and current_date > start_date:\n",
    "        pushshift_posts = get_pushshift_posts(subreddit_name, start_date, current_date, limit=1000)\n",
    "        if not pushshift_posts:\n",
    "            print(\"No more posts found or error occurred.\")\n",
    "            break\n",
    "        \n",
    "        for post_data in pushshift_posts:\n",
    "            post_date = datetime.utcfromtimestamp(post_data['created_utc'])\n",
    "            if post_date < start_date or len(posts) >= max_posts:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Fetch full post data from Reddit API\n",
    "                full_post = reddit.submission(id=post_data['id'])\n",
    "                posts.append(RedditPost(full_post))\n",
    "                pbar.update(1)\n",
    "            except praw.exceptions.PRAWException as e:\n",
    "                print(f\"Error fetching post {post_data['id']}: {e}\")\n",
    "            \n",
    "            time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "        \n",
    "        current_date = datetime.utcfromtimestamp(pushshift_posts[-1]['created_utc'])\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Retrieved {len(posts)} posts. Oldest post date: {datetime.utcfromtimestamp(posts[-1].created_utc) if posts else 'N/A'}\")\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    n_years = 5  # Change this to the desired number of years\n",
    "\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=n_years*365)\n",
    "    \n",
    "    print(f\"Fetching posts from r/{subreddit_name} for the period: {start_date.date()} to {end_date.date()}\")\n",
    "    reddit_posts = get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date)\n",
    "\n",
    "    # Create filename with date range\n",
    "    filename = f\"{subreddit_name}_{start_date.date()}_{end_date.date()}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    print(\"\\nSample of fetched data:\")\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date, max_posts=100000):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    posts = []\n",
    "    pbar = tqdm(total=max_posts, desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    # Convert dates to timestamps for Reddit's API\n",
    "    start_timestamp = int(start_date.timestamp())\n",
    "    end_timestamp = int(end_date.timestamp())\n",
    "    \n",
    "    # Use empty string to search for all posts\n",
    "    for post in subreddit.search('', sort='new', syntax='lucene', \n",
    "                                 time_filter='all', limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if post_date < start_date:\n",
    "            break\n",
    "        if start_date <= post_date <= end_date:\n",
    "            posts.append(RedditPost(post))\n",
    "            pbar.update(1)\n",
    "            if len(posts) >= max_posts:\n",
    "                break\n",
    "        \n",
    "        time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Retrieved {len(posts)} posts. Oldest post date: {datetime.utcfromtimestamp(posts[-1].created_utc) if posts else 'N/A'}\")\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    n_years = 5  # Change this to the desired number of years\n",
    "\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=n_years*365)\n",
    "    \n",
    "    print(f\"Fetching posts from r/{subreddit_name} for the period: {start_date.date()} to {end_date.date()}\")\n",
    "    reddit_posts = get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date)\n",
    "\n",
    "    # Create filename with date range\n",
    "    filename = f\"{subreddit_name}_{start_date.date()}_{end_date.date()}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    print(\"\\nSample of fetched data:\")\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date, max_posts=100000):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    posts = []\n",
    "    pbar = tqdm(total=max_posts, desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    for post in subreddit.top('all', limit=None):\n",
    "        post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "        if start_date <= post_date <= end_date:\n",
    "            posts.append(RedditPost(post))\n",
    "            pbar.update(1)\n",
    "            if len(posts) >= max_posts:\n",
    "                break\n",
    "        \n",
    "        time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Retrieved {len(posts)} posts. Oldest post date: {datetime.utcfromtimestamp(posts[-1].created_utc) if posts else 'N/A'}\")\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    n_years = 5  # Change this to the desired number of years\n",
    "\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=n_years*365)\n",
    "    \n",
    "    print(f\"Fetching posts from r/{subreddit_name} for the period: {start_date.date()} to {end_date.date()}\")\n",
    "    reddit_posts = get_subreddit_posts_for_period(reddit, subreddit_name, start_date, end_date)\n",
    "\n",
    "    # Create filename with date range\n",
    "    filename = f\"{subreddit_name}_{start_date.date()}_{end_date.date()}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    print(\"\\nSample of fetched data:\")\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_posts_from_urls(reddit, urls):\n",
    "    posts = []\n",
    "    pbar = tqdm(total=len(urls), desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            submission = reddit.submission(url=url)\n",
    "            posts.append(RedditPost(submission))\n",
    "            pbar.update(1)\n",
    "            time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "        except praw.exceptions.PRAWException as e:\n",
    "            print(f\"Error fetching post {url}: {e}\")\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Retrieved {len(posts)} posts.\")\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    \n",
    "    # List of URLs to fetch\n",
    "    urls = [\"https://www.reddit.com/r/PioneerDJ/comments/1ekyq9f/how_to_load_music_from_pc_to_cdj_using_an_xdjxz/\",\n",
    "        # Add more URLs as needed\n",
    "    ]\n",
    "    \n",
    "    print(f\"Fetching {len(urls)} posts from Reddit\")\n",
    "    reddit_posts = get_posts_from_urls(reddit, urls)\n",
    "\n",
    "    # Create filename with current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"Reddit_posts_{timestamp}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', 'Reddit_posts', filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(reddit_posts)} posts to file...\")\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    print(f\"Saved {len(reddit_posts)} posts with comments to {file_path}\")\n",
    "\n",
    "    # Print a sample of the data\n",
    "    print(\"\\nSample of fetched data:\")\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_all_post_urls(reddit, subreddit_name, limit=None):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    urls = []\n",
    "    \n",
    "    # We'll use 'new' to get the most recent posts first\n",
    "    for post in tqdm(subreddit.new(limit=limit), desc=\"Fetching posts\", unit=\"post\"):\n",
    "        url = f\"https://www.reddit.com{post.permalink}\"\n",
    "        urls.append(url)\n",
    "        time.sleep(0.1)  # Sleep for 100ms between requests to respect rate limits\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def save_urls_to_file(urls, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for url in urls:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "def main():\n",
    "    reddit = initialize_reddit()\n",
    "    subreddit_name = 'PioneerDJ'\n",
    "    \n",
    "    print(f\"Fetching post URLs from r/{subreddit_name}\")\n",
    "    urls = get_all_post_urls(reddit, subreddit_name)\n",
    "\n",
    "    # Create filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{subreddit_name}_post_urls_{timestamp}.txt\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join('..', 'data', subreddit_name, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving {len(urls)} URLs to file...\")\n",
    "    save_urls_to_file(urls, file_path)\n",
    "\n",
    "    print(f\"Saved {len(urls)} URLs to {file_path}\")\n",
    "\n",
    "    # Print a sample of the URLs\n",
    "    print(\"\\nSample of fetched URLs:\")\n",
    "    for url in urls[:5]:\n",
    "        print(url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] input_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: input_file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andres/opt/anaconda3/envs/filo-nlp/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "# ... (RedditComment and RedditPost classes remain the same)\n",
    "\n",
    "def initialize_reddit():\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "    )\n",
    "\n",
    "def get_posts_from_urls(reddit, urls):\n",
    "    posts = []\n",
    "    pbar = tqdm(total=len(urls), desc=\"Fetching posts\", unit=\"post\")\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            submission = reddit.submission(url=url)\n",
    "            posts.append(RedditPost(submission))\n",
    "            pbar.update(1)\n",
    "            time.sleep(0.1)  # Sleep for 100ms between requests\n",
    "        except praw.exceptions.PRAWException as e:\n",
    "            print(f\"Error fetching post {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error fetching post {url}: {e}\")\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Retrieved {len(posts)} posts.\")\n",
    "    return posts\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump([post.to_dict() for post in posts], f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(posts)} posts with comments to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving posts to file: {e}\")\n",
    "\n",
    "def load_urls_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except IOError as e:\n",
    "        print(f\"Error loading URLs from file: {e}\")\n",
    "        return []\n",
    "\n",
    "def main(input_file):\n",
    "    reddit = initialize_reddit()\n",
    "    \n",
    "    urls = load_urls_from_file(input_file)\n",
    "    \n",
    "    if not urls:\n",
    "        print(\"No URLs found. Please make sure the file exists and contains URLs.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Fetching {len(urls)} posts from Reddit\")\n",
    "    reddit_posts = get_posts_from_urls(reddit, urls)\n",
    "\n",
    "    # Create filename with current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"Reddit_posts_{timestamp}.json\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(os.path.dirname(input_file), filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    save_posts(reddit_posts, file_path)\n",
    "\n",
    "    # Print a sample of the data\n",
    "    print(\"\\nSample of fetched data:\")\n",
    "    for post in reddit_posts[:3]:\n",
    "        print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Date: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "        print(f\"Content: {post.selftext[:200]}...\" if post.is_self else \"This is a link post.\")\n",
    "        print(f\"Number of comments: {len(post.comments)}\")\n",
    "        if post.comments:\n",
    "            print(f\"First comment: {post.comments[0].body[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Fetch Reddit posts from URLs in a file.\")\n",
    "    parser.add_argument(\"input_file\", help=\"Path to the file containing Reddit post URLs\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args.input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filo-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
